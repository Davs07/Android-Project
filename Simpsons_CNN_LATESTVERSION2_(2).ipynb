{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Davs07/Android-Project/blob/main/Simpsons_CNN_LATESTVERSION2_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX8gZlVyCCbz"
      },
      "source": [
        "# ACTIVIDAD GRUPAL 3: REDES NEURONALES CONVOLUCIONALES\n",
        "\n",
        "---\n",
        "\n",
        "En esta actividad, vamos a trabajar con Convolutional Neural Networks para resolver un problema de clasificación de imágenes. En particular, vamos a clasificar imágenes de personajes de la conocida serie de los Simpsons.\n",
        "\n",
        "Como las CNN profundas son un tipo de modelo bastante avanzado y computacionalmente costoso, se recomienda hacer la práctica en Google Colaboratory con soporte para GPUs. En [este enlace](https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d) se explica cómo activar un entorno con GPUs. *Nota: para leer las imágenes y estandarizarlas al mismo tamaño se usa la librería opencv. Esta ĺibrería está ya instalada en el entorno de Colab, pero si trabajas de manera local tienes que instalarla.*\n",
        "\n",
        "<center><img src=\"https://i.imgur.com/i8zIGqX.jpg\" style=\"text-align: center\" height=\"300px\"></center>\n",
        "\n",
        "El dataset a utilizar consiste en imágenes de personajes de los Simpsons extraídas directamente de capítulos de la serie. Este dataset ha sido recopilado por [Alexandre Attia](http://www.alexattia.fr/) y es más complejo que el dataset de  MNIST que hemos utilizado hasta ahora. Aparte de tener más clases (vamos a utilizar los 18 personajes con más imágenes), los personajes pueden aparecer en distintas poses, en distintas posiciones de la imagen o con otros personajes en pantalla (si bien el personaje a clasificar siempre aparece en la posición predominante).\n",
        "\n",
        "El dataset de training puede ser descargado desde aquí:\n",
        "\n",
        "[Training data](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219337&authkey=AMzI92bJPx8Sd60) (~500MB)\n",
        "\n",
        "Por otro lado, el dataset de test puede ser descargado de aquí:\n",
        "\n",
        "[Test data](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219341&authkey=ANnjK3Uq1FhuAe8) (~10MB)\n",
        "\n",
        "Antes de empezar la práctica, se recomienda descargar las imágenes y echarlas un vistazo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nwgxHmm9X1ss",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b014716-1e08-424a-83f2-9dacdcbad4f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip install gradio -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI274F8LQC59"
      },
      "source": [
        "## Carga de los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "D7tKOZ9BFfki"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import keras\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import tensorflow as tf\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iw0apA7ruy1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "138819a8-79b0-46b3-970f-a87d3a702a8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219337&authkey=AMzI92bJPx8Sd60\n",
            "\u001b[1m375144448/523789527\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 1us/step"
          ]
        }
      ],
      "source": [
        "# Primero, bajamos los datos de entrenamiento\n",
        "keras.utils.get_file(fname=\"simpsons_train.tar.gz\",\n",
        "                     origin=\"https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219337&authkey=AMzI92bJPx8Sd60\")\n",
        "\n",
        "# Descomprimimos el archivo\n",
        "!tar -xzf /root/.keras/datasets/simpsons_train.tar.gz -C /root/.keras/datasets\n",
        "\n",
        "\n",
        "# Hacemos lo mismo con los datos de test\n",
        "keras.utils.get_file(fname=\"simpsons_test.tar.gz\",\n",
        "                     origin=\"https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219341&authkey=ANnjK3Uq1FhuAe8\")\n",
        "!tar -xzf /root/.keras/datasets/simpsons_test.tar.gz -C /root/.keras/datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HIhp512sPUJ"
      },
      "outputs": [],
      "source": [
        "# Descomprimimos el archivo en tmp para visualizar\n",
        "# !tar -xzf /root/.keras/datasets/simpsons_train.tar.gz -C /tmp/simpsons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMFhe3COFwSD"
      },
      "outputs": [],
      "source": [
        "# Esta variable contiene un mapeo de número de clase a personaje.\n",
        "# Utilizamos sólo los 18 personajes del dataset que tienen más imágenes.\n",
        "MAP_CHARACTERS = {\n",
        "    0: 'abraham_grampa_simpson', 1: 'apu_nahasapeemapetilon', 2: 'bart_simpson',\n",
        "    3: 'charles_montgomery_burns', 4: 'chief_wiggum', 5: 'comic_book_guy', 6: 'edna_krabappel',\n",
        "    7: 'homer_simpson', 8: 'kent_brockman', 9: 'krusty_the_clown', 10: 'lisa_simpson',\n",
        "    11: 'marge_simpson', 12: 'milhouse_van_houten', 13: 'moe_szyslak',\n",
        "    14: 'ned_flanders', 15: 'nelson_muntz', 16: 'principal_skinner', 17: 'sideshow_bob'\n",
        "}\n",
        "\n",
        "# Vamos a standarizar todas las imágenes a tamaño 64x64\n",
        "IMG_SIZE = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bJ0NsbCbupF"
      },
      "outputs": [],
      "source": [
        "def load_train_set(dirname, map_characters, verbose=True):\n",
        "    \"\"\"Esta función carga los datos de training en imágenes.\n",
        "\n",
        "    Como las imágenes tienen tamaños distintas, utilizamos la librería opencv\n",
        "    para hacer un resize y adaptarlas todas a tamaño IMG_SIZE x IMG_SIZE.\n",
        "\n",
        "    Args:\n",
        "        dirname: directorio completo del que leer los datos\n",
        "        map_characters: variable de mapeo entre labels y personajes\n",
        "        verbose: si es True, muestra información de las imágenes cargadas\n",
        "\n",
        "    Returns:\n",
        "        X, y: X es un array con todas las imágenes cargadas con tamaño\n",
        "                IMG_SIZE x IMG_SIZE\n",
        "              y es un array con las labels de correspondientes a cada imagen\n",
        "    \"\"\"\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "    for label, character in map_characters.items():\n",
        "        files = os.listdir(os.path.join(dirname, character))\n",
        "        images = [file for file in files if file.endswith(\"jpg\")]\n",
        "        if verbose:\n",
        "          print(\"Leyendo {} imágenes encontradas de {}\".format(len(images), character))\n",
        "        for image_name in images:\n",
        "            image = cv2.imread(os.path.join(dirname, character, image_name))\n",
        "            image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "            image = image.reshape(IMG_SIZE, IMG_SIZE, 1)\n",
        "            X_train.append(cv2.resize(image,(IMG_SIZE, IMG_SIZE)))\n",
        "            y_train.append(label)\n",
        "    return np.array(X_train), np.array(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NslxhnnDK6uA"
      },
      "outputs": [],
      "source": [
        "def load_test_set(dirname, map_characters, verbose=True):\n",
        "    \"\"\"Esta función funciona de manera equivalente a la función load_train_set\n",
        "    pero cargando los datos de test.\"\"\"\n",
        "    X_test = []\n",
        "    y_test = []\n",
        "    reverse_dict = {v: k for k, v in map_characters.items()}\n",
        "    for filename in glob.glob(dirname + '/*.*'):\n",
        "        char_name = \"_\".join(filename.split('/')[-1].split('_')[:-1])\n",
        "        if char_name in reverse_dict:\n",
        "            image = cv2.imread(filename)\n",
        "            image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "            image = image.reshape(IMG_SIZE, IMG_SIZE, 1)\n",
        "            X_test.append(image)\n",
        "            y_test.append(reverse_dict[char_name])\n",
        "    if verbose:\n",
        "        print(\"Leídas {} imágenes de test\".format(len(X_test)))\n",
        "    return np.array(X_test), np.array(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVWqKxFcbwTu"
      },
      "outputs": [],
      "source": [
        "# Cargamos los datos. Si no estás trabajando en colab, cambia los paths por\n",
        "# los de los ficheros donde hayas descargado los datos.\n",
        "DATASET_TRAIN_PATH_COLAB = \"/root/.keras/datasets/simpsons\"\n",
        "DATASET_TEST_PATH_COLAB = \"/root/.keras/datasets/simpsons_testset\"\n",
        "\n",
        "X, y = load_train_set(DATASET_TRAIN_PATH_COLAB, MAP_CHARACTERS)\n",
        "X_t, y_t = load_test_set(DATASET_TEST_PATH_COLAB, MAP_CHARACTERS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GY4vTFyfffv"
      },
      "outputs": [],
      "source": [
        "# Vamos a barajar aleatoriamente los datos. Esto es importante ya que si no\n",
        "# lo hacemos y, por ejemplo, cogemos el 20% de los datos finales como validation\n",
        "# set, estaremos utilizando solo un pequeño número de personajes, ya que\n",
        "# las imágenes se leen secuencialmente personaje a personaje.\n",
        "perm = np.random.permutation(len(X))\n",
        "X, y = X[perm], y[perm]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOoQ7_0GrylF"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(X_t[11].reshape(IMG_SIZE, IMG_SIZE), cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBbmz9DMhVhc"
      },
      "source": [
        "## Ejercicio\n",
        "\n",
        "Utilizando Convolutional Neural Networks con Keras, entrenar un clasificador que sea capaz de reconocer personajes en imágenes de los Simpsons con una accuracy en el dataset de test de, al menos, **90%**. Redactar un informe analizando varias de las alternativas probadas y los resultados obtenidos.\n",
        "\n",
        "A continuación se detallan una serie de aspectos orientativos que podrían ser analizados en su informe (no es necesario tratar todos ellos ni mucho menos, esto son ideas orientativas de aspectos que pueden explorar):\n",
        "\n",
        "*   Análisis de los datos a utilizar.\n",
        "*   Análisis de resultados, obtención de métricas de *precision* y *recall* por clase y análisis de qué clases obtienen mejores o peores resultados.\n",
        "*   Análisis visual de los errores de la red. ¿Qué tipo de imágenes o qué personajes dan más problemas a nuestro modelo?\n",
        "*   Comparación de modelos CNNs con un modelo de Fully Connected para este problema.\n",
        "*   Utilización de distintas arquitecturas CNNs, comentando aspectos como su profundidad, hiperparámetros utilizados, optimizador, uso de técnicas de regularización, *batch normalization*, etc.\n",
        "*   [ *algo más difícil* ] Utilización de *data augmentation*. Esto puede conseguirse con la clase [ImageDataGenerator](https://keras.io/preprocessing/image/#imagedatagenerator-class) de Keras.\n",
        "\n",
        "Notas:\n",
        "* Recuerda partir los datos en training/validation para tener una buena estimación de los valores que nuestro modelo tendrá en los datos de test, así como comprobar que no estamos cayendo en overfitting. Una posible partición puede ser 80 / 20.\n",
        "* No es necesario mostrar en el notebook las trazas de entrenamiento de todos los modelos entrenados, si bien una buena idea seria guardar gráficas de esos entrenamientos para el análisis. Sin embargo, **se debe mostrar el entrenamiento completo del mejor modelo obtenido y la evaluación de los datos de test con este modelo**.\n",
        "* Las imágenes **no están normalizadas**. Hay que normalizarlas como hemos hecho en trabajos anteriores.\n",
        "* El test set del problema tiene imágenes un poco más \"fáciles\", por lo que es posible encontrarse con métricas en el test set bastante mejores que en el training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KvJCO0w2u1V"
      },
      "outputs": [],
      "source": [
        "# Normalización de los datos (valores entre 0 y 1)\n",
        "X = X / 255.0\n",
        "X_t = X_t / 255.0\n",
        "\n",
        "# Cambiamos el formato de las etiquetas a one-hot encoding\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_train_one_hot = to_categorical(y, num_classes=len(MAP_CHARACTERS))\n",
        "y_test_one_hot = to_categorical(y_t, num_classes=len(MAP_CHARACTERS))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSFDehp73ehh"
      },
      "outputs": [],
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "import datetime\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5l__BD1h2jFG"
      },
      "outputs": [],
      "source": [
        "# Montar Google Drive para guardar los logs\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Definir la ruta en Google Drive para guardar los logs de TensorBoard\n",
        "log_dir_base = \"/content/drive/My Drive/Colab Notebooks/tensorboard_logs/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNPFJiek7mdG"
      },
      "outputs": [],
      "source": [
        "!ls /content/drive/My\\ Drive/Colab\\ Notebooks/tensorboard_logs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfNb_wkV2sS9"
      },
      "outputs": [],
      "source": [
        "# Crear el directorio si no existe\n",
        "if not os.path.exists(log_dir_base):\n",
        "    os.makedirs(log_dir_base)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNu3v_ppzIMM"
      },
      "source": [
        "# Modelo 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EosrqPwT20TL"
      },
      "outputs": [],
      "source": [
        "# ----------ARQUITECTURA PRIMER MODELO-------------\n",
        "\n",
        "# Definir el primer modelo CNN\n",
        "cnn_model = Sequential()\n",
        "\n",
        "# Primera capa convolucional\n",
        "cnn_model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n",
        "cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Segunda capa convolucional\n",
        "cnn_model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Tercera capa convolucional\n",
        "cnn_model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Aplanar la salida\n",
        "cnn_model.add(Flatten())\n",
        "\n",
        "# Capa completamente conectada\n",
        "cnn_model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# Capa de salida\n",
        "cnn_model.add(Dense(18, activation='softmax'))  # 18 clases de salida\n",
        "\n",
        "# Compilar el modelo\n",
        "cnn_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZK7k6XPzLvA"
      },
      "source": [
        "# Modelo 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5GW3tPrxmE9"
      },
      "outputs": [],
      "source": [
        "# ----------ARQUITECTURA SEGUNDO MODELO-------------\n",
        "\n",
        "# Definir el segundo modelo CNN\n",
        "cnn_model2 = Sequential()\n",
        "\n",
        "# Primera capa convolucional\n",
        "cnn_model2.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n",
        "cnn_model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Segunda capa convolucional\n",
        "cnn_model2.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "cnn_model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Tercera capa convolucional\n",
        "cnn_model2.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "cnn_model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Añadir DropOut\n",
        "cnn_model2.add(Dropout(0.5))\n",
        "\n",
        "# Aplanar la salida\n",
        "cnn_model2.add(Flatten())\n",
        "\n",
        "# Capa completamente conectada\n",
        "cnn_model2.add(Dense(256, activation='relu'))\n",
        "\n",
        "# Capa de salida\n",
        "cnn_model2.add(Dense(18, activation='softmax'))  # 18 clases de salida\n",
        "\n",
        "# Compilar el segundo modelo\n",
        "cnn_model2.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2c-Xt1zyzocQ"
      },
      "outputs": [],
      "source": [
        "# ----------CONFIGURAR TENSORBOARD-------------\n",
        "\n",
        "# Definir los directorios de los logs para TensorBoard\n",
        "log_dir_cnn_model = \"logs/cnn_model/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "log_dir_cnn_model2 = \"logs/cnn_model2/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "# Crear callbacks para TensorBoard\n",
        "tensorboard_callback_cnn = TensorBoard(log_dir=log_dir_cnn_model, histogram_freq=1)\n",
        "tensorboard_callback_cnn2 = TensorBoard(log_dir=log_dir_cnn_model2, histogram_freq=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIKUJLtc06Vg"
      },
      "source": [
        "# sasa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ij0pDgnJ--Wh"
      },
      "outputs": [],
      "source": [
        "# ----------ENTRENAMIENTO DEL PRIMER MODELO-------------\n",
        "\n",
        "# Entrenar el primer modelo con TensorBoard\n",
        "history_cnn = cnn_model.fit(\n",
        "    X,\n",
        "    y_train_one_hot,\n",
        "    epochs=25,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_t, y_test_one_hot),\n",
        "    callbacks=[tensorboard_callback_cnn]\n",
        ")\n",
        "\n",
        "# Evaluar el primer modelo\n",
        "cnn_model.evaluate(X_t, y_test_one_hot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6B2LBgI08HC"
      },
      "outputs": [],
      "source": [
        "# ----------ENTRENAMIENTO DEL SEGUNDO MODELO-------------\n",
        "\n",
        "# Entrenar el segundo modelo con TensorBoard\n",
        "history_cnn2 = cnn_model2.fit(\n",
        "    X,\n",
        "    y_train_one_hot,\n",
        "    epochs=25,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_t, y_test_one_hot),\n",
        "    callbacks=[tensorboard_callback_cnn2]\n",
        ")\n",
        "\n",
        "# Evaluar el segundo modelo\n",
        "cnn_model2.evaluate(X_t, y_test_one_hot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIywOfgX1Ctp"
      },
      "outputs": [],
      "source": [
        "# ----------VISUALIZAR TENSORBOARD-------------\n",
        "\n",
        "# Cargar la extensión de TensorBoard en Google Colab\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Ejecutar TensorBoard\n",
        "%tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues, figsize=(10, 8)):\n",
        "    \"\"\"\n",
        "    Esta función imprime y dibuja la matriz de confusión.\n",
        "    La normalización se puede aplicar estableciendo `normalize=True`.\n",
        "    `figsize` controla el tamaño de la imagen.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    # Configurar el tamaño del gráfico\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    # Mostrar la matriz de confusión\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=90)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()\n",
        "\n",
        "# Calcular la matriz de confusión\n",
        "y_pred = cnn_model.predict(X_t)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_test_one_hot, axis=1)\n",
        "\n",
        "confusion_mtx = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "\n",
        "# Obtener las etiquetas de las clases\n",
        "class_labels = list(MAP_CHARACTERS.values())\n",
        "\n",
        "# Mostrar la matriz de confusión con etiquetas y un tamaño de gráfico más grande\n",
        "plot_confusion_matrix(confusion_mtx, classes=class_labels, title='Confusion matrix', figsize=(12, 10))\n"
      ],
      "metadata": {
        "id": "sQ4IJePdhdgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjzaRv759ILu"
      },
      "source": [
        "# Guardar el Modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdnG2vnb-Jll"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Guardar el primer modelo\n",
        "cnn_model.save('cnn_model.h5')\n",
        "\n",
        "# Guardar el segundo modelo\n",
        "cnn_model2.save('cnn_model2.h5')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Wgrq-Fi-OeA"
      },
      "source": [
        "# Probar el Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzJcvIzA_ZOW"
      },
      "outputs": [],
      "source": [
        "test_image_folder = '/content/drive/MyDrive/Colab Notebooks/simpsons_imagenes/Simpson'\n",
        "!ls /content/drive/MyDrive/Colab\\ Notebooks/simpsons_imagenes/Simpson"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mO4rHkF7-U5K"
      },
      "outputs": [],
      "source": [
        "# prompt: Carga el modelo, procesa las imágenes para testear (de una carpeta con esta ruta drive/MyDrive/Colab Notebooks/simpsons_ds/simpsons_test), predice la clase de cada imagen y muestras los resultados\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from google.colab import drive\n",
        "\n",
        "# Montar Google Drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Cargar el modelo entrenado\n",
        "model = load_model('cnn_model.h5')  # O 'cnn_model2.h5' si quieres usar el segundo modelo\n",
        "\n",
        "# Ruta de la carpeta con las imágenes de prueba\n",
        "test_image_folder =  '/content/drive/MyDrive/Colab Notebooks/simpsons_imagenes/Simpson'\n",
        "\n",
        "# Procesar las imágenes de prueba\n",
        "def process_test_images(folder_path):\n",
        "    images = []\n",
        "    image_paths = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
        "            img_path = os.path.join(folder_path, filename)\n",
        "            img = cv2.imread(img_path)\n",
        "            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Convertir a escala de grises\n",
        "            img = img.reshape(IMG_SIZE, IMG_SIZE, 1)    # Cambiar forma para CNN\n",
        "            images.append(img)\n",
        "            image_paths.append(img_path)  # Guardar las rutas de las imágenes\n",
        "    return np.array(images), image_paths\n",
        "\n",
        "# Cargar y procesar las imágenes de prueba\n",
        "X_test_custom, image_paths = process_test_images(test_image_folder)\n",
        "X_test_custom = X_test_custom / 255.0  # Normalizar las imágenes\n",
        "\n",
        "# Realizar la predicción\n",
        "predictions = model.predict(X_test_custom)\n",
        "\n",
        "# Mostrar los resultados con la ruta y la clase predicha\n",
        "for i, prediction in enumerate(predictions):\n",
        "    predicted_class = np.argmax(prediction)  # Obtener la clase predicha\n",
        "    predicted_character = MAP_CHARACTERS[predicted_class]  # Mapear a personaje\n",
        "    print(f\"Imagen: {image_paths[i]}\")\n",
        "    print(f\"Personaje predicho: {predicted_character}\")\n",
        "    print(\"-------------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: eneseñame las imagenes predichas\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Mostrar las imágenes predichas\n",
        "for i, prediction in enumerate(predictions):\n",
        "    predicted_class = np.argmax(prediction)\n",
        "    predicted_character = MAP_CHARACTERS[predicted_class]\n",
        "    plt.imshow(X_test_custom[i].reshape(IMG_SIZE, IMG_SIZE), cmap='gray')\n",
        "    plt.title(f\"Predicted: {predicted_character}\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "X0hFhmoEHNax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79SA-07uJaL-"
      },
      "outputs": [],
      "source": [
        "# prompt: Carga el modelo, procesa las imágenes para testear (de una carpeta con esta ruta drive/MyDrive/Colab Notebooks/simpsons_ds/simpsons_test), predice la clase de cada imagen y muestras los resultados\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from google.colab import drive\n",
        "\n",
        "# Montar Google Drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Cargar el modelo entrenado\n",
        "model = load_model('cnn_model2.h5')  # O 'cnn_model2.h5' si quieres usar el segundo modelo\n",
        "\n",
        "# Ruta de la carpeta con las imágenes de prueba\n",
        "test_image_folder =  '/content/drive/MyDrive/Colab Notebooks/simpsons_imagenes/Simpson'\n",
        "\n",
        "# Procesar las imágenes de prueba\n",
        "def process_test_images(folder_path):\n",
        "    images = []\n",
        "    image_paths = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
        "            img_path = os.path.join(folder_path, filename)\n",
        "            img = cv2.imread(img_path)\n",
        "            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Convertir a escala de grises\n",
        "            img = img.reshape(IMG_SIZE, IMG_SIZE, 1)    # Cambiar forma para CNN\n",
        "            images.append(img)\n",
        "            image_paths.append(img_path)  # Guardar las rutas de las imágenes\n",
        "    return np.array(images), image_paths\n",
        "\n",
        "# Cargar y procesar las imágenes de prueba\n",
        "X_test_custom, image_paths = process_test_images(test_image_folder)\n",
        "X_test_custom = X_test_custom / 255.0  # Normalizar las imágenes\n",
        "\n",
        "# Realizar la predicción\n",
        "predictions = model.predict(X_test_custom)\n",
        "\n",
        "# Mostrar los resultados con la ruta y la clase predicha\n",
        "for i, prediction in enumerate(predictions):\n",
        "    predicted_class = np.argmax(prediction)  # Obtener la clase predicha\n",
        "    predicted_character = MAP_CHARACTERS[predicted_class]  # Mapear a personaje\n",
        "    print(f\"Imagen: {image_paths[i]}\")\n",
        "    print(f\"Personaje predicho: {predicted_character}\")\n",
        "    print(\"-------------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: eneseñame las imagenes predichas\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Mostrar las imágenes predichas\n",
        "for i, prediction in enumerate(predictions):\n",
        "    predicted_class = np.argmax(prediction)\n",
        "    predicted_character = MAP_CHARACTERS[predicted_class]\n",
        "    plt.imshow(X_test_custom[i].reshape(IMG_SIZE, IMG_SIZE), cmap='gray')\n",
        "    plt.title(f\"Predicted: {predicted_character}\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "FM2jo0gWKaGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbQsBAh9FJ8c"
      },
      "source": [
        "# Exportación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LNwIKgQFEIG"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflowjs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMqxuzDMFNIF"
      },
      "outputs": [],
      "source": [
        "!mkdir carpeta_salida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0q2TQV8FRQb"
      },
      "outputs": [],
      "source": [
        "!tensorflowjs_converter --input_format keras cnn_model2.h5 carpeta_salida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QS4w3Z0MHYlv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}